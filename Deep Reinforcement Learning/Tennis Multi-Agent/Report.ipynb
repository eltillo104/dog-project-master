{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "I decided to solve this project of the Tennis environment by using MADDPG. I used an actor and critic for each agent in this environment. I experimented with many different hyperparameters and architectures for more than one month. I started using two different agents that learned from their own states and actions using DDPG without taking into account the opponent's state. I got a maximum average score of +0.07. So I noticed I had to approach this with MADDPG, I had a hard time understanding this algorithm from their research paper but after careful study I realized the idea behind it. The two critics had to obtain states and actions from both agents to approach the optimal Q-value for the actors.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "I decided to use the two states of each agent to pass them through the critic. With this I realized that the Critic was learning from the two states but approaching an optimal Q-value with the action and reward of the actor from that agent. This meant that each agent considered the state of his opponent but was rewarded on his own action at that particular timestep. Each critic and actor had its own target network to follow. Having 4 different target networks and 4 different local networks. The architecture I used for the Critic and Actor was 400 nodes for the first layer with relu activation and 300 nodes for the second layer with relu activation as well. I used a replay buffer of 1e6 to save the experiences of the agents. A batch size of 1024, a discount factor of 99%, soft update for the target networks to 0.1%, learning rate for the actors of 1e-4, and learning rate for the critics of 3e-4. Both of the states, actions, rewards, next_states, and dones were saved in the same tuple. I reorganized the values for each agent every update of the networks. The environment was solved in 6500 episodes acquiring an average score of +0.51.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "The network architecture was used from the original DDPG learning algorithm. I obtained really good results on the past project using this architecture. The relu activation was used due to it's great effectiveness in different algorithms in Deep Learning. The tanh activation function was used on the last layer on the actors to obtain actions between -1 and 1. The critic architecture grabs the states and actions and concatenates them to obtain the Q-Value to be used on the actors. Relu activations were used to obtain good results.\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "The learning algorithm grabs random batches of tuples saved from every episode to the replay buffer to be passed through actors and critics. The batches of tuples are reshaped to have different sets of tuples for agent 1 and agent 2 separately. The actors and critics for both agents are computed similarly.\n",
    "\n",
    "### Update Critics\n",
    "The next actions of each agent is obtained by passing the next states of that same agent through the actor target network of that agent. The next Q target for each agent is calculated by passing the next states of both of the agents and the next actions for the selected agent. The Q-target for each critic is obtained by using the Bellman equation using the rewards from the selected agent, dones for the selected agent and the Q-targets for the selected agent. We then calculate the expected Q-value for the selected critic and calculate it by passing through the selected local critic network the states of both agents and the actions of the selected agent. I then calculate the mean squared error loss by passing through the expected Q-value and the target Q value from the selected agent. I make the gradient be zero to avoid the exploding gradient problem. I calculate the critic loss from the selected agent anduse Adam optimizer.\n",
    "\n",
    "### Update Actors\n",
    "The predicted actions for each actor is obtained by passing the states for the selected agent through the local actor of that agent. The actor loss is calculated by obtaining the negative of the mean from the local critic of the selected agent. To obtain the local critic of the selected agent I use the states of both agents and the predicted actions of the selectec actor. I zero the gradient of the selected actor optimizer, I calculate the loss of the selected actor and use Adam optimizer.\n",
    "\n",
    "### Soft Update\n",
    "I use the soft update for each critic and actor to their corresponding target networks.\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "I really think I can make the learning process faster by adjusting the learning rates of the actors and critics. Using a decaying random process were it starts being exploratory and finishes exploiting the environment. Normalizing the rewards, using prioritized experience replay, and changing the network architecture for both actors and critics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
